{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E_3PjGTNUYLr",
        "outputId": "c40381c2-b825-4b04-985e-f37ffd310f16"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "folder_path = \"/content/drive/My Drive/CICIDS 2017/\"  # Update with correct path\n",
        "\n",
        "# List all CSV files in the folder\n",
        "csv_files = [f for f in os.listdir(folder_path) if f.endswith('.csv')]\n",
        "\n",
        "# Read and combine all CSV files\n",
        "df_list = [pd.read_csv(os.path.join(folder_path, file)) for file in csv_files]\n",
        "\n",
        "# Concatenate all DataFrames into one\n",
        "df = pd.concat(df_list, ignore_index=True)\n",
        "\n",
        "print(df.head())  # Display first few rows\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CAVS5h42U-wG",
        "outputId": "a46b708c-cfc6-4b7b-eb49-47c20f6d116f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    Destination Port   Flow Duration   Total Fwd Packets  \\\n",
            "0              54865               3                   2   \n",
            "1              55054             109                   1   \n",
            "2              55055              52                   1   \n",
            "3              46236              34                   1   \n",
            "4              54863               3                   2   \n",
            "\n",
            "    Total Backward Packets  Total Length of Fwd Packets  \\\n",
            "0                        0                           12   \n",
            "1                        1                            6   \n",
            "2                        1                            6   \n",
            "3                        1                            6   \n",
            "4                        0                           12   \n",
            "\n",
            "    Total Length of Bwd Packets   Fwd Packet Length Max  \\\n",
            "0                             0                       6   \n",
            "1                             6                       6   \n",
            "2                             6                       6   \n",
            "3                             6                       6   \n",
            "4                             0                       6   \n",
            "\n",
            "    Fwd Packet Length Min   Fwd Packet Length Mean   Fwd Packet Length Std  \\\n",
            "0                       6                      6.0                     0.0   \n",
            "1                       6                      6.0                     0.0   \n",
            "2                       6                      6.0                     0.0   \n",
            "3                       6                      6.0                     0.0   \n",
            "4                       6                      6.0                     0.0   \n",
            "\n",
            "   ...   min_seg_size_forward  Active Mean   Active Std   Active Max  \\\n",
            "0  ...                     20          0.0          0.0            0   \n",
            "1  ...                     20          0.0          0.0            0   \n",
            "2  ...                     20          0.0          0.0            0   \n",
            "3  ...                     20          0.0          0.0            0   \n",
            "4  ...                     20          0.0          0.0            0   \n",
            "\n",
            "    Active Min  Idle Mean   Idle Std   Idle Max   Idle Min   Label  \n",
            "0            0        0.0        0.0          0          0  BENIGN  \n",
            "1            0        0.0        0.0          0          0  BENIGN  \n",
            "2            0        0.0        0.0          0          0  BENIGN  \n",
            "3            0        0.0        0.0          0          0  BENIGN  \n",
            "4            0        0.0        0.0          0          0  BENIGN  \n",
            "\n",
            "[5 rows x 79 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# importing required libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import pickle\n",
        "from os import path\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "from sklearn import metrics\n",
        "from sklearn import preprocessing\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.neural_network import MLPClassifier"
      ],
      "metadata": {
        "id": "0bsC4qdtVFMC"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive"
      ],
      "metadata": {
        "id": "wNA0I8UHVKnO"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bNCPVpKSVPba",
        "outputId": "0a6707bf-3da4-4746-eee4-4d3d3adec19f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls \"/content/drive/My Drive/CICIDS 2017/\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bfYvrLmYVVDH",
        "outputId": "49362fdf-31ee-4cd6-d0fc-99b6d182cc9b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv  Monday-WorkingHours.pcap_ISCX.csv\n",
            "Friday-WorkingHours-Morning.pcap_ISCX.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv('/content/drive/My Drive/CICIDS 2017/Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv')"
      ],
      "metadata": {
        "id": "McNKJI3HVe-k"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "\n",
        "# --- **IMPORTANT:** Replace with the actual path to your CSV file ---\n",
        "file_path = '/content/drive/My Drive/CICIDS 2017/Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv'\n",
        "# Example: file_path = '/content/drive/My Drive/CICIDS 2017/Monday-workingHours.pcap_ISCX.csv'\n",
        "\n",
        "try:\n",
        "    data = pd.read_csv(file_path)\n",
        "    print(\"Dataset loaded successfully.\")\n",
        "    print(\"\\nFirst 5 rows of the dataset:\")\n",
        "    print(data.head())\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: The file '{file_path}' was not found. Please check the file path.\")\n",
        "    exit()\n",
        "\n",
        "# Select numerical features\n",
        "numerical_features = data.select_dtypes(include=np.number).columns\n",
        "\n",
        "print(\"\\nIdentified numerical features:\", numerical_features.tolist())\n",
        "\n",
        "# --- **Handle infinite and very large values** ---\n",
        "print(\"\\nChecking for infinite values in numerical columns...\")\n",
        "if len(numerical_features) > 0:\n",
        "    infinite_cols = data[numerical_features].columns[np.isinf(data[numerical_features]).any()].tolist()\n",
        "    print(\"Columns with infinite values:\", infinite_cols)\n",
        "    data[numerical_features].replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "    print(\"Infinite values in numerical columns replaced with NaN.\")\n",
        "\n",
        "    large_threshold = 1e15  # Adjust as needed\n",
        "    print(f\"\\nChecking for very large values (>{large_threshold}) in numerical columns...\")\n",
        "    for col in numerical_features:\n",
        "        if pd.api.types.is_numeric_dtype(data[col]):  # Ensure column is numeric before comparison\n",
        "            if (data[col].astype(float) > large_threshold).any():\n",
        "                print(f\"Column '{col}' has very large values.\")\n",
        "                data[col].where(data[col].astype(float) <= large_threshold, np.nan, inplace=True)\n",
        "    print(\"Very large values in numerical columns replaced with NaN.\")\n",
        "\n",
        "    # --- **Handle NaN values (imputation)** ---\n",
        "    print(\"\\nHandling NaN values in numerical columns (filling with mean)...\")\n",
        "    data[numerical_features].fillna(data[numerical_features].mean(), inplace=True)\n",
        "    print(\"NaN values in numerical columns filled with the mean.\")\n",
        "\n",
        "    # Create copies for scaled data\n",
        "    data_minmax = data.copy()\n",
        "    data_standard = data.copy()\n",
        "\n",
        "    # Normalize using Min-Max scaling on numerical features\n",
        "    min_max_scaler = MinMaxScaler()\n",
        "    data_minmax[numerical_features] = min_max_scaler.fit_transform(data_minmax[numerical_features])\n",
        "\n",
        "    print(\"\\nFirst 5 rows after Min-Max Normalization:\")\n",
        "    print(data_minmax.head())\n",
        "\n",
        "    # Standardize using Z-score scaling on numerical features\n",
        "    standard_scaler = StandardScaler()\n",
        "    data_standard[numerical_features] = standard_scaler.fit_transform(data_standard[numerical_features])\n",
        "\n",
        "    print(\"\\nFirst 5 rows after Standardization (Z-score):\")\n",
        "    print(data_standard.head())\n",
        "\n",
        "else:\n",
        "    print(\"\\nNo numerical features found in the dataset. Skipping normalization and standardization.\")\n",
        "\n",
        "print(\"\\nData pre-processing using Min-Max Normalization and Standardization (Z-score) complete (if numerical features were found).\")\n",
        "\n",
        "# --- Important Notes: ---\n",
        "# 1. **File Path:** Ensure the `file_path` variable at the beginning of the script correctly points to your CSV file.\n",
        "# 2. **Numerical Features:** The code automatically identifies numerical features. Review the output of \"Identified numerical features\" to ensure it's correct.\n",
        "# 3. **Handling Infinite and Large Values:**\n",
        "#    - Infinite values (inf, -inf) are replaced with NaN.\n",
        "#    - Very large values (exceeding a threshold) are also replaced with NaN. Adjust `large_threshold` if needed.\n",
        "# 4. **Missing Values:** NaN values in numerical columns are filled with the mean of each column. Consider other imputation methods if appropriate for your data.\n",
        "# 5. **Scaling:** Min-Max Normalization scales features to the range [0, 1]. Standardization (Z-score) scales features to have a mean of 0 and a standard deviation of 1.\n",
        "# 6. **Separate Scaled DataFrames:** The normalized data is stored in `data_minmax`, and the standardized data is in `data_standard`. The original `data` DataFrame is also preserved (after handling infinite/large/NaN values).\n",
        "# 7. **No Numerical Features:** The code now handles the case where no numerical features are found in the dataset."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yzn77bZAV3hL",
        "outputId": "a9331c56-b0d1-4497-f22f-43eaa3a4518d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset loaded successfully.\n",
            "\n",
            "First 5 rows of the dataset:\n",
            "    Destination Port   Flow Duration   Total Fwd Packets  \\\n",
            "0              54865               3                   2   \n",
            "1              55054             109                   1   \n",
            "2              55055              52                   1   \n",
            "3              46236              34                   1   \n",
            "4              54863               3                   2   \n",
            "\n",
            "    Total Backward Packets  Total Length of Fwd Packets  \\\n",
            "0                        0                           12   \n",
            "1                        1                            6   \n",
            "2                        1                            6   \n",
            "3                        1                            6   \n",
            "4                        0                           12   \n",
            "\n",
            "    Total Length of Bwd Packets   Fwd Packet Length Max  \\\n",
            "0                             0                       6   \n",
            "1                             6                       6   \n",
            "2                             6                       6   \n",
            "3                             6                       6   \n",
            "4                             0                       6   \n",
            "\n",
            "    Fwd Packet Length Min   Fwd Packet Length Mean   Fwd Packet Length Std  \\\n",
            "0                       6                      6.0                     0.0   \n",
            "1                       6                      6.0                     0.0   \n",
            "2                       6                      6.0                     0.0   \n",
            "3                       6                      6.0                     0.0   \n",
            "4                       6                      6.0                     0.0   \n",
            "\n",
            "   ...   min_seg_size_forward  Active Mean   Active Std   Active Max  \\\n",
            "0  ...                     20          0.0          0.0            0   \n",
            "1  ...                     20          0.0          0.0            0   \n",
            "2  ...                     20          0.0          0.0            0   \n",
            "3  ...                     20          0.0          0.0            0   \n",
            "4  ...                     20          0.0          0.0            0   \n",
            "\n",
            "    Active Min  Idle Mean   Idle Std   Idle Max   Idle Min   Label  \n",
            "0            0        0.0        0.0          0          0  BENIGN  \n",
            "1            0        0.0        0.0          0          0  BENIGN  \n",
            "2            0        0.0        0.0          0          0  BENIGN  \n",
            "3            0        0.0        0.0          0          0  BENIGN  \n",
            "4            0        0.0        0.0          0          0  BENIGN  \n",
            "\n",
            "[5 rows x 79 columns]\n",
            "\n",
            "Identified numerical features: [' Destination Port', ' Flow Duration', ' Total Fwd Packets', ' Total Backward Packets', 'Total Length of Fwd Packets', ' Total Length of Bwd Packets', ' Fwd Packet Length Max', ' Fwd Packet Length Min', ' Fwd Packet Length Mean', ' Fwd Packet Length Std', 'Bwd Packet Length Max', ' Bwd Packet Length Min', ' Bwd Packet Length Mean', ' Bwd Packet Length Std', 'Flow Bytes/s', ' Flow Packets/s', ' Flow IAT Mean', ' Flow IAT Std', ' Flow IAT Max', ' Flow IAT Min', 'Fwd IAT Total', ' Fwd IAT Mean', ' Fwd IAT Std', ' Fwd IAT Max', ' Fwd IAT Min', 'Bwd IAT Total', ' Bwd IAT Mean', ' Bwd IAT Std', ' Bwd IAT Max', ' Bwd IAT Min', 'Fwd PSH Flags', ' Bwd PSH Flags', ' Fwd URG Flags', ' Bwd URG Flags', ' Fwd Header Length', ' Bwd Header Length', 'Fwd Packets/s', ' Bwd Packets/s', ' Min Packet Length', ' Max Packet Length', ' Packet Length Mean', ' Packet Length Std', ' Packet Length Variance', 'FIN Flag Count', ' SYN Flag Count', ' RST Flag Count', ' PSH Flag Count', ' ACK Flag Count', ' URG Flag Count', ' CWE Flag Count', ' ECE Flag Count', ' Down/Up Ratio', ' Average Packet Size', ' Avg Fwd Segment Size', ' Avg Bwd Segment Size', ' Fwd Header Length.1', 'Fwd Avg Bytes/Bulk', ' Fwd Avg Packets/Bulk', ' Fwd Avg Bulk Rate', ' Bwd Avg Bytes/Bulk', ' Bwd Avg Packets/Bulk', 'Bwd Avg Bulk Rate', 'Subflow Fwd Packets', ' Subflow Fwd Bytes', ' Subflow Bwd Packets', ' Subflow Bwd Bytes', 'Init_Win_bytes_forward', ' Init_Win_bytes_backward', ' act_data_pkt_fwd', ' min_seg_size_forward', 'Active Mean', ' Active Std', ' Active Max', ' Active Min', 'Idle Mean', ' Idle Std', ' Idle Max', ' Idle Min']\n",
            "\n",
            "Checking for infinite values in numerical columns...\n",
            "Columns with infinite values: ['Flow Bytes/s', ' Flow Packets/s']\n",
            "Infinite values in numerical columns replaced with NaN.\n",
            "\n",
            "Checking for very large values (>1000000000000000.0) in numerical columns...\n",
            "Column 'Flow Bytes/s' has very large values.\n",
            "Column ' Flow Packets/s' has very large values.\n",
            "Very large values in numerical columns replaced with NaN.\n",
            "\n",
            "Handling NaN values in numerical columns (filling with mean)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-95f9f0588c13>:28: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  data[numerical_features].replace([np.inf, -np.inf], np.nan, inplace=True)\n",
            "<ipython-input-8-95f9f0588c13>:37: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  data[col].where(data[col].astype(float) <= large_threshold, np.nan, inplace=True)\n",
            "<ipython-input-8-95f9f0588c13>:37: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  data[col].where(data[col].astype(float) <= large_threshold, np.nan, inplace=True)\n",
            "<ipython-input-8-95f9f0588c13>:42: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  data[numerical_features].fillna(data[numerical_features].mean(), inplace=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NaN values in numerical columns filled with the mean.\n",
            "\n",
            "First 5 rows after Min-Max Normalization:\n",
            "    Destination Port   Flow Duration   Total Fwd Packets  \\\n",
            "0           0.837225    3.333335e-08            0.000518   \n",
            "1           0.840109    9.166671e-07            0.000000   \n",
            "2           0.840124    4.416669e-07            0.000000   \n",
            "3           0.705548    2.916668e-07            0.000000   \n",
            "4           0.837194    3.333335e-08            0.000518   \n",
            "\n",
            "    Total Backward Packets  Total Length of Fwd Packets  \\\n",
            "0                  0.00000                     0.000066   \n",
            "1                  0.00034                     0.000033   \n",
            "2                  0.00034                     0.000033   \n",
            "3                  0.00034                     0.000033   \n",
            "4                  0.00000                     0.000066   \n",
            "\n",
            "    Total Length of Bwd Packets   Fwd Packet Length Max  \\\n",
            "0                      0.000000                0.000514   \n",
            "1                      0.000001                0.000514   \n",
            "2                      0.000001                0.000514   \n",
            "3                      0.000001                0.000514   \n",
            "4                      0.000000                0.000514   \n",
            "\n",
            "    Fwd Packet Length Min   Fwd Packet Length Mean   Fwd Packet Length Std  \\\n",
            "0                0.004076                 0.001552                     0.0   \n",
            "1                0.004076                 0.001552                     0.0   \n",
            "2                0.004076                 0.001552                     0.0   \n",
            "3                0.004076                 0.001552                     0.0   \n",
            "4                0.004076                 0.001552                     0.0   \n",
            "\n",
            "   ...   min_seg_size_forward  Active Mean   Active Std   Active Max  \\\n",
            "0  ...               0.384615          0.0          0.0          0.0   \n",
            "1  ...               0.384615          0.0          0.0          0.0   \n",
            "2  ...               0.384615          0.0          0.0          0.0   \n",
            "3  ...               0.384615          0.0          0.0          0.0   \n",
            "4  ...               0.384615          0.0          0.0          0.0   \n",
            "\n",
            "    Active Min  Idle Mean   Idle Std   Idle Max   Idle Min   Label  \n",
            "0          0.0        0.0        0.0        0.0        0.0  BENIGN  \n",
            "1          0.0        0.0        0.0        0.0        0.0  BENIGN  \n",
            "2          0.0        0.0        0.0        0.0        0.0  BENIGN  \n",
            "3          0.0        0.0        0.0        0.0        0.0  BENIGN  \n",
            "4          0.0        0.0        0.0        0.0        0.0  BENIGN  \n",
            "\n",
            "[5 rows x 79 columns]\n",
            "\n",
            "First 5 rows after Standardization (Z-score):\n",
            "    Destination Port   Flow Duration   Total Fwd Packets  \\\n",
            "0           2.327831       -0.515210           -0.186406   \n",
            "1           2.337398       -0.515207           -0.251245   \n",
            "2           2.337449       -0.515209           -0.251245   \n",
            "3           1.891022       -0.515209           -0.251245   \n",
            "4           2.327730       -0.515210           -0.186406   \n",
            "\n",
            "    Total Backward Packets  Total Length of Fwd Packets  \\\n",
            "0                -0.210191                    -0.285426   \n",
            "1                -0.164225                    -0.287273   \n",
            "2                -0.164225                    -0.287273   \n",
            "3                -0.164225                    -0.287273   \n",
            "4                -0.210191                    -0.285426   \n",
            "\n",
            "    Total Length of Bwd Packets   Fwd Packet Length Max  \\\n",
            "0                     -0.151982               -0.285676   \n",
            "1                     -0.151829               -0.285676   \n",
            "2                     -0.151829               -0.285676   \n",
            "3                     -0.151829               -0.285676   \n",
            "4                     -0.151982               -0.285676   \n",
            "\n",
            "    Fwd Packet Length Min   Fwd Packet Length Mean   Fwd Packet Length Std  \\\n",
            "0               -0.133981                -0.314576               -0.269507   \n",
            "1               -0.133981                -0.314576               -0.269507   \n",
            "2               -0.133981                -0.314576               -0.269507   \n",
            "3               -0.133981                -0.314576               -0.269507   \n",
            "4               -0.133981                -0.314576               -0.269507   \n",
            "\n",
            "   ...   min_seg_size_forward  Active Mean   Active Std   Active Max  \\\n",
            "0  ...               -0.35585    -0.231634    -0.061512    -0.231146   \n",
            "1  ...               -0.35585    -0.231634    -0.061512    -0.231146   \n",
            "2  ...               -0.35585    -0.231634    -0.061512    -0.231146   \n",
            "3  ...               -0.35585    -0.231634    -0.061512    -0.231146   \n",
            "4  ...               -0.35585    -0.231634    -0.061512    -0.231146   \n",
            "\n",
            "    Active Min  Idle Mean   Idle Std   Idle Max   Idle Min   Label  \n",
            "0    -0.226482  -0.472345  -0.283137  -0.478364  -0.391071  BENIGN  \n",
            "1    -0.226482  -0.472345  -0.283137  -0.478364  -0.391071  BENIGN  \n",
            "2    -0.226482  -0.472345  -0.283137  -0.478364  -0.391071  BENIGN  \n",
            "3    -0.226482  -0.472345  -0.283137  -0.478364  -0.391071  BENIGN  \n",
            "4    -0.226482  -0.472345  -0.283137  -0.478364  -0.391071  BENIGN  \n",
            "\n",
            "[5 rows x 79 columns]\n",
            "\n",
            "Data pre-processing using Min-Max Normalization and Standardization (Z-score) complete (if numerical features were found).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense\n",
        "\n",
        "# --- **IMPORTANT:** Replace with the actual path to your CSV file ---\n",
        "file_path ='/content/drive/My Drive/CICIDS 2017/Friday-WorkingHours-Morning.pcap_ISCX.csv'\n",
        "# Example: file_path = '/content/drive/My Drive/CICIDS 2017/Monday-workingHours.pcap_ISCX.csv'\n",
        "\n",
        "# --- Load and check data in a separate cell ---\n",
        "try:\n",
        "    data = pd.read_csv(file_path)\n",
        "    print(\"Dataset loaded successfully.\")\n",
        "    print(\"\\nFirst 5 rows of the dataset:\")\n",
        "    print(data.head())\n",
        "\n",
        "    # --- Start a new cell for the remaining code ---\n",
        "\n",
        "    # Identify the target variable column name\n",
        "    target_column = 'Label'  # **IMPORTANT:** Replace 'Label' with the actual name of your target variable column\n",
        "\n",
        "    # Select numerical features for normalization and standardization\n",
        "    # --- Check if the target column exists in the DataFrame ---\n",
        "    if target_column in data.columns:\n",
        "        numerical_features = data.select_dtypes(include=np.number).columns\n",
        "\n",
        "        # --- Now continue with the rest of your feature selection and preprocessing code ---\n",
        "\n",
        "        print(\"\\nIdentified numerical features:\", numerical_features.tolist())\n",
        "        # --- **Handle infinite and very large values** ---\n",
        "        print(\"\\nChecking for infinite values in numerical columns...\")\n",
        "        if len(numerical_features) > 0:\n",
        "            infinite_cols = data[numerical_features].columns[np.isinf(data[numerical_features]).any()].tolist()\n",
        "            print(\"Columns with infinite values:\", infinite_cols)\n",
        "            data[numerical_features].replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "            print(\"Infinite values in numerical columns replaced with NaN.\")\n",
        "\n",
        "            large_threshold = 1e15  # Adjust as needed\n",
        "            print(f\"\\nChecking for very large values (>{large_threshold}) in numerical columns...\")\n",
        "            for col in numerical_features:\n",
        "                if pd.api.types.is_numeric_dtype(data[col]):  # Ensure column is numeric before comparison\n",
        "                    if (data[col].astype(float) > large_threshold).any():\n",
        "                        print(f\"Column '{col}' has very large values.\")\n",
        "                        data[col].where(data[col].astype(float) <= large_threshold, np.nan, inplace=True)\n",
        "            print(\"Very large values in numerical columns replaced with NaN.\")\n",
        "\n",
        "            # --- **Handle NaN values (imputation)** ---\n",
        "            print(\"\\nHandling NaN values in numerical columns (filling with mean)...\")\n",
        "            data[numerical_features].fillna(data[numerical_features].mean(), inplace=True)\n",
        "            print(\"NaN values in numerical columns filled with the mean.\")\n",
        "\n",
        "            # Create copies for scaled data\n",
        "            data_standard = data.copy()\n",
        "\n",
        "            # Standardize using Z-score scaling on numerical features\n",
        "            standard_scaler = StandardScaler()\n",
        "            data_standard[numerical_features] = standard_scaler.fit_transform(data_standard[numerical_features])\n",
        "\n",
        "            print(\"\\nFirst 5 rows after Standardization (Z-score):\")\n",
        "            print(data_standard.head())\n",
        "\n",
        "            # --- Feature Selection ---\n",
        "            # Check if the target column exists in the DataFrame\n",
        "            if target_column not in data_standard.columns:\n",
        "                print(f\"\\nError: Target variable column '{target_column}' not found in the preprocessed data.\")\n",
        "            else:\n",
        "                # Separate features (X) and target (y)\n",
        "                X = data_standard.drop(target_column, axis=1)\n",
        "                y = data_standard[target_column]\n",
        "\n",
        "                # Split data into training and testing sets\n",
        "                X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "                # Decision Tree\n",
        "                dt_classifier = DecisionTreeClassifier(random_state=42)\n",
        "                dt_classifier.fit(X_train, y_train)\n",
        "                dt_y_pred = dt_classifier.predict(X_test)\n",
        "                dt_accuracy = accuracy_score(y_test, dt_y_pred)\n",
        "                print(f\"\\nDecision Tree Accuracy: {dt_accuracy}\")\n",
        "\n",
        "                # Feature importances from Decision Tree\n",
        "                dt_feature_importances = dt_classifier.feature_importances_\n",
        "                dt_important_features = pd.Series(dt_feature_importances, index=X.columns).sort_values(ascending=False)\n",
        "                print(\"\\nDecision Tree Feature Importances:\")\n",
        "                print(dt_important_features)\n",
        "\n",
        "                # Random Forest\n",
        "                rf_classifier = RandomForestClassifier(random_state=42)\n",
        "                rf_classifier.fit(X_train, y_train)\n",
        "                rf_y_pred = rf_classifier.predict(X_test)\n",
        "                rf_accuracy = accuracy_score(y_test, rf_y_pred)\n",
        "                print(f\"\\nRandom Forest Accuracy: {rf_accuracy}\")\n",
        "\n",
        "                # Feature importances from Random Forest\n",
        "                rf_feature_importances = rf_classifier.feature_importances_\n",
        "                rf_important_features = pd.Series(rf_feature_importances, index=X.columns).sort_values(ascending=False)\n",
        "                print(\"\\nRandom Forest Feature Importances:\")\n",
        "                print(rf_important_features)\n",
        "\n",
        "                # --- CNN Feature Selection (Illustrative Example) ---\n",
        "                # CNNs are not directly used for feature selection in the same way as trees.\n",
        "                # They learn features during training. You can examine the learned filters or\n",
        "                # weights to understand feature importance but it's a more complex process.\n",
        "                # Here is an illustrative example using the output of the first layer of a simple CNN\n",
        "                # which might represent the extracted features:\n",
        "\n",
        "                # Check if the number of features is greater than 0 before proceeding with CNN\n",
        "                if X_train.shape[1] > 0:\n",
        "                    # Example (requires conversion of data into suitable format for CNN)\n",
        "                    # Assume data reshaped into num_samples, timesteps, features for CNN\n",
        "                    # You might need to reshape or otherwise process your data\n",
        "                    X_cnn = X_train.values.reshape(X_train.shape[0], 1, X_train.shape[1]) # Example reshape\n",
        "\n",
        "                    # Determine the number of unique classes in the target variable for the output layer\n",
        "                    num_classes = len(np.unique(y_train))\n",
        "\n",
        "                    model = keras.Sequential([\n",
        "                        Conv1D(filters=32, kernel_size=3, activation='relu', input_shape=(1, X_train.shape[1])),\n",
        "                        MaxPooling1D(pool_size=2),\n",
        "                        Flatten(),\n",
        "                        Dense(num_classes, activation='softmax') # Output layer based on the number of classes\n",
        "                    ])\n",
        "                    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "                    model.fit(X_cnn, y_train, epochs=10, verbose=0) # Train for limited epochs for illustration\n",
        "\n",
        "                    # Extract first layer feature maps\n",
        "                    intermediate_layer_model = keras.Model(inputs=model.input, outputs=model.layers[0].output)\n",
        "                    intermediate_output = intermediate_layer_model.predict(X_cnn)\n",
        "\n",
        "                    print(f\"\\nShape of CNN's first layer output: {intermediate_output.shape}\")\n",
        "                    print(\"\\nCNN: Examining the learned filters or weights of the first layer can provide insights into which features the CNN finds important. This requires further analysis of the `model.layers[0].get_weights()`.\")\n",
        "                else:\n",
        "                    print(\"\\nCNN: No features available for CNN model.\")\n",
        "\n",
        "        else:\n",
        "            print(\"\\nNo numerical features found in the dataset. Skipping feature selection.\")\n",
        "\n",
        "        print(\"\\nFeature selection using Decision Tree, Random Forest, and CNN (illustrative) complete (if numerical features and target variable were found).\")\n",
        "\n",
        "    else:\n",
        "        print(f\"Error: Target column '{target_column}' not found in the dataset.\")\n",
        "        exit()\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: The file '{file_path}' was not found. Please check the file path.\")\n",
        "    exit()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wl2XnLNbWEwm",
        "outputId": "2725a313-fcfd-4c1f-924c-32ed796a3b3e"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset loaded successfully.\n",
            "\n",
            "First 5 rows of the dataset:\n",
            "    Destination Port   Flow Duration   Total Fwd Packets  \\\n",
            "0               3268       112740690                  32   \n",
            "1                389       112740560                  32   \n",
            "2                  0       113757377                 545   \n",
            "3               5355          100126                  22   \n",
            "4                  0           54760                   4   \n",
            "\n",
            "    Total Backward Packets  Total Length of Fwd Packets  \\\n",
            "0                       16                         6448   \n",
            "1                       16                         6448   \n",
            "2                        0                            0   \n",
            "3                        0                          616   \n",
            "4                        0                            0   \n",
            "\n",
            "    Total Length of Bwd Packets   Fwd Packet Length Max  \\\n",
            "0                          1152                     403   \n",
            "1                          5056                     403   \n",
            "2                             0                       0   \n",
            "3                             0                      28   \n",
            "4                             0                       0   \n",
            "\n",
            "    Fwd Packet Length Min   Fwd Packet Length Mean   Fwd Packet Length Std  \\\n",
            "0                       0                    201.5              204.724205   \n",
            "1                       0                    201.5              204.724205   \n",
            "2                       0                      0.0                0.000000   \n",
            "3                      28                     28.0                0.000000   \n",
            "4                       0                      0.0                0.000000   \n",
            "\n",
            "   ...   min_seg_size_forward   Active Mean    Active Std   Active Max  \\\n",
            "0  ...                     32  3.594286e+02  1.199802e+01          380   \n",
            "1  ...                     32  3.202857e+02  1.574499e+01          330   \n",
            "2  ...                      0  9.361829e+06  7.324646e+06     18900000   \n",
            "3  ...                     32  0.000000e+00  0.000000e+00            0   \n",
            "4  ...                      0  0.000000e+00  0.000000e+00            0   \n",
            "\n",
            "    Active Min   Idle Mean      Idle Std   Idle Max   Idle Min   Label  \n",
            "0          343  16100000.0  4.988048e+05   16400000   15400000  BENIGN  \n",
            "1          285  16100000.0  4.987937e+05   16400000   15400000  BENIGN  \n",
            "2           19  12200000.0  6.935824e+06   20800000    5504997  BENIGN  \n",
            "3            0         0.0  0.000000e+00          0          0  BENIGN  \n",
            "4            0         0.0  0.000000e+00          0          0  BENIGN  \n",
            "\n",
            "[5 rows x 79 columns]\n",
            "Error: Target column 'Label' not found in the dataset.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Reshape\n",
        "\n",
        "# --- **IMPORTANT:** Replace with the actual path to your CSV file ---\n",
        "file_path = '/content/drive/My Drive/CICIDS 2017/Friday-WorkingHours-Morning.pcap_ISCX.csv'\n",
        "# Example: file_path = '/content/drive/My Drive/CICIDS 2017/Monday-workingHours.pcap_ISCX.csv'\n",
        "\n",
        "# --- **SOLUTION:** Correct the target variable column name ---\n",
        "# The traceback indicates a KeyError: \"['Label'] not found in axis\".\n",
        "# The error message printed previously also shows the available columns.\n",
        "# Based on the printed list of columns, the target variable column name is ' Label' (note the leading space).\n",
        "# Replace 'Label' below with the actual target column name including the space.\n",
        "target_column = ' Label'  # **REPLACE 'Label' WITH THE ACTUAL TARGET COLUMN NAME (including space)**\n",
        "\n",
        "try:\n",
        "    data = pd.read_csv(file_path)\n",
        "    print(\"Dataset loaded successfully.\")\n",
        "    print(\"\\nFirst 5 rows of the dataset:\")\n",
        "    print(data.head())\n",
        "\n",
        "    # Separate features (X) and target (y)\n",
        "    # --- **SOLUTION:** Check if the target column exists BEFORE trying to drop it ---\n",
        "    if target_column not in data.columns:\n",
        "        print(f\"\\nError: Target variable column '{target_column}' not found in the dataset columns:\")\n",
        "        print(data.columns.tolist())  # Print the actual column names for debugging\n",
        "        print(\"Please update the 'target_column' variable with the correct name (including any leading/trailing spaces).\")\n",
        "        exit()\n",
        "    X = data.drop(target_column, axis=1)\n",
        "    y = data[target_column]\n",
        "\n",
        "    # Identify numerical and categorical features\n",
        "    numerical_features = X.select_dtypes(include=np.number).columns\n",
        "    categorical_features = X.select_dtypes(include=['object', 'category']).columns\n",
        "\n",
        "    # --- Preprocessing ---\n",
        "    # Handle missing values (impute numerical with mean, categorical with mode)\n",
        "    for col in numerical_features:\n",
        "        X[col].fillna(X[col].mean(), inplace=True)\n",
        "    for col in categorical_features:\n",
        "        if not X[col].empty:  # Check if the column is not empty before taking mode\n",
        "            X[col].fillna(X[col].mode()[0], inplace=True)\n",
        "        else:\n",
        "            print(f\"Warning: Categorical column '{col}' is empty and cannot be imputed with mode.\")\n",
        "\n",
        "    # Encode categorical features\n",
        "    X = pd.get_dummies(X, columns=categorical_features, drop_first=True)\n",
        "\n",
        "    # Split data into training and testing sets\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "    # Encode the target variable if it's not already numerical\n",
        "    label_encoder = LabelEncoder()\n",
        "    y_train_encoded = label_encoder.fit_transform(y_train)\n",
        "    y_test_encoded = label_encoder.transform(y_test)\n",
        "    num_classes = len(np.unique(y_train_encoded))\n",
        "    print(f\"\\nNumber of classes: {num_classes}\")\n",
        "\n",
        "    # --- **SOLUTION:** Handle potential infinite or very large values before scaling ---\n",
        "    print(\"\\nChecking for infinite or very large values in numerical features...\")\n",
        "    numerical_cols_x_train = X_train.select_dtypes(include=np.number).columns\n",
        "    numerical_cols_x_test = X_test.select_dtypes(include=np.number).columns\n",
        "\n",
        "    for col in numerical_cols_x_train:\n",
        "        X_train[col] = np.where(np.isinf(X_train[col]), np.nan, X_train[col])\n",
        "        X_train[col] = np.where(np.abs(X_train[col]) > 1e10, np.nan, X_train[col])  # Adjust threshold as needed\n",
        "    for col in numerical_cols_x_test:\n",
        "        X_test[col] = np.where(np.isinf(X_test[col]), np.nan, X_test[col])\n",
        "        X_test[col] = np.where(np.abs(X_test[col]) > 1e10, np.nan, X_test[col])  # Adjust threshold as needed\n",
        "\n",
        "    # Impute any remaining NaNs after handling inf/large values\n",
        "    for col in numerical_cols_x_train:\n",
        "        X_train[col].fillna(X_train[col].mean(), inplace=True)\n",
        "    for col in numerical_cols_x_test:\n",
        "        X_test[col].fillna(X_test[col].mean(), inplace=True)\n",
        "\n",
        "    # Scale numerical features\n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X_train)\n",
        "    X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "    # Convert scaled data back to DataFrame for easier handling (optional)\n",
        "    X_train_scaled_df = pd.DataFrame(X_train_scaled, columns=X_train.columns)\n",
        "    X_test_scaled_df = pd.DataFrame(X_test_scaled, columns=X_test.columns)\n",
        "\n",
        "    # --- Multiclassification Models ---\n",
        "\n",
        "    print(\"\\n--- Decision Tree ---\")\n",
        "    dt_classifier = DecisionTreeClassifier(random_state=42)\n",
        "    dt_classifier.fit(X_train_scaled_df, y_train_encoded)\n",
        "    dt_y_pred = dt_classifier.predict(X_test_scaled_df)\n",
        "    dt_accuracy = accuracy_score(y_test_encoded, dt_y_pred)\n",
        "    print(f\"Accuracy: {dt_accuracy:.4f}\")\n",
        "    print(\"Classification Report:\\n\", classification_report(y_test_encoded, dt_y_pred))\n",
        "\n",
        "    print(\"\\n--- Random Forest ---\")\n",
        "    rf_classifier = RandomForestClassifier(random_state=42)\n",
        "    rf_classifier.fit(X_train_scaled_df, y_train_encoded)\n",
        "    rf_y_pred = rf_classifier.predict(X_test_scaled_df)\n",
        "    rf_accuracy = accuracy_score(y_test_encoded, rf_y_pred)\n",
        "    print(f\"Accuracy: {rf_accuracy:.4f}\")\n",
        "    print(\"Classification Report:\\n\", classification_report(y_test_encoded, rf_y_pred))\n",
        "\n",
        "    print(\"\\n--- Logistic Regression ---\")\n",
        "    lr_classifier = LogisticRegression(random_state=42, solver='liblinear', multi_class='ovr', max_iter=1000)\n",
        "    lr_classifier.fit(X_train_scaled_df, y_train_encoded)\n",
        "    lr_y_pred = lr_classifier.predict(X_test_scaled_df)\n",
        "    lr_accuracy = accuracy_score(y_test_encoded, lr_y_pred)\n",
        "    print(f\"Accuracy: {lr_accuracy:.4f}\")\n",
        "    print(\"Classification Report:\\n\", classification_report(y_test_encoded, lr_y_pred))\n",
        "\n",
        "    print(\"\\n--- Support Vector Machine (SVM) ---\")\n",
        "    svm_classifier = SVC(random_state=42, kernel='rbf', C=1.0)\n",
        "    svm_classifier.fit(X_train_scaled_df, y_train_encoded)\n",
        "    svm_y_pred = svm_classifier.predict(X_test_scaled_df)\n",
        "    svm_accuracy = accuracy_score(y_test_encoded, svm_y_pred)\n",
        "    print(f\"Accuracy: {svm_accuracy:.4f}\")\n",
        "    print(\"Classification Report:\\n\", classification_report(y_test_encoded, svm_y_pred))\n",
        "\n",
        "    print(\"\\n--- Gaussian Naive Bayes ---\")\n",
        "    gnb_classifier = GaussianNB()\n",
        "    gnb_classifier.fit(X_train_scaled_df, y_train_encoded)\n",
        "    gnb_y_pred = gnb_classifier.predict(X_test_scaled_df)\n",
        "    gnb_accuracy = accuracy_score(y_test_encoded, gnb_y_pred)\n",
        "    print(f\"Accuracy: {gnb_accuracy:.4f}\")\n",
        "    print(\"Classification Report:\\n\", classification_report(y_test_encoded, gnb_y_pred))\n",
        "\n",
        "    print(\"\\n--- Convolutional Neural Network (CNN) ---\")\n",
        "    # Reshape input data for CNN (assuming 1 time step)\n",
        "    X_train_cnn = X_train_scaled.reshape(X_train_scaled.shape[0], X_train_scaled.shape[1], 1)\n",
        "    X_test_cnn = X_test_scaled.reshape(X_test_scaled.shape[0], X_test_scaled.shape[1], 1)\n",
        "\n",
        "    cnn_model = keras.Sequential([\n",
        "        Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(X_train_cnn.shape[1], 1)),\n",
        "        MaxPooling1D(pool_size=2),\n",
        "        Conv1D(filters=128, kernel_size=3, activation='relu'),\n",
        "        MaxPooling1D(pool_size=2),\n",
        "        Flatten(),\n",
        "        Dense(128, activation='relu'),\n",
        "        Dense(num_classes, activation='softmax')\n",
        "    ])\n",
        "\n",
        "    cnn_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "    cnn_model.summary()\n",
        "\n",
        "    # Train the CNN\n",
        "    epochs = 10\n",
        "    batch_size = 32\n",
        "    cnn_model.fit(X_train_cnn, y_train_encoded, epochs=epochs, batch_size=batch_size, verbose=0)\n",
        "\n",
        "    # Evaluate the CNN\n",
        "    cnn_loss, cnn_accuracy = cnn_model.evaluate(X_test_cnn, y_test_encoded, verbose=0)\n",
        "    print(f\"CNN Accuracy: {cnn_accuracy:.4f}\")\n",
        "\n",
        "    print(\"\\nMulticlassification using various techniques complete.\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"\\nError: The file '{file_path}' was not found. Please check the file path.\")\n",
        "    exit()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "7nHhyQvEbAx3",
        "outputId": "78c75e62-1120-48c6-8029-d315728c1506"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset loaded successfully.\n",
            "\n",
            "First 5 rows of the dataset:\n",
            "    Destination Port   Flow Duration   Total Fwd Packets  \\\n",
            "0               3268       112740690                  32   \n",
            "1                389       112740560                  32   \n",
            "2                  0       113757377                 545   \n",
            "3               5355          100126                  22   \n",
            "4                  0           54760                   4   \n",
            "\n",
            "    Total Backward Packets  Total Length of Fwd Packets  \\\n",
            "0                       16                         6448   \n",
            "1                       16                         6448   \n",
            "2                        0                            0   \n",
            "3                        0                          616   \n",
            "4                        0                            0   \n",
            "\n",
            "    Total Length of Bwd Packets   Fwd Packet Length Max  \\\n",
            "0                          1152                     403   \n",
            "1                          5056                     403   \n",
            "2                             0                       0   \n",
            "3                             0                      28   \n",
            "4                             0                       0   \n",
            "\n",
            "    Fwd Packet Length Min   Fwd Packet Length Mean   Fwd Packet Length Std  \\\n",
            "0                       0                    201.5              204.724205   \n",
            "1                       0                    201.5              204.724205   \n",
            "2                       0                      0.0                0.000000   \n",
            "3                      28                     28.0                0.000000   \n",
            "4                       0                      0.0                0.000000   \n",
            "\n",
            "   ...   min_seg_size_forward   Active Mean    Active Std   Active Max  \\\n",
            "0  ...                     32  3.594286e+02  1.199802e+01          380   \n",
            "1  ...                     32  3.202857e+02  1.574499e+01          330   \n",
            "2  ...                      0  9.361829e+06  7.324646e+06     18900000   \n",
            "3  ...                     32  0.000000e+00  0.000000e+00            0   \n",
            "4  ...                      0  0.000000e+00  0.000000e+00            0   \n",
            "\n",
            "    Active Min   Idle Mean      Idle Std   Idle Max   Idle Min   Label  \n",
            "0          343  16100000.0  4.988048e+05   16400000   15400000  BENIGN  \n",
            "1          285  16100000.0  4.987937e+05   16400000   15400000  BENIGN  \n",
            "2           19  12200000.0  6.935824e+06   20800000    5504997  BENIGN  \n",
            "3            0         0.0  0.000000e+00          0          0  BENIGN  \n",
            "4            0         0.0  0.000000e+00          0          0  BENIGN  \n",
            "\n",
            "[5 rows x 79 columns]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-10-6adba7cad1cf>:49: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  X[col].fillna(X[col].mean(), inplace=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Number of classes: 2\n",
            "\n",
            "Checking for infinite or very large values in numerical features...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-10-6adba7cad1cf>:83: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  X_train[col].fillna(X_train[col].mean(), inplace=True)\n",
            "<ipython-input-10-6adba7cad1cf>:85: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  X_test[col].fillna(X_test[col].mean(), inplace=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Decision Tree ---\n",
            "Accuracy: 0.9997\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00     37814\n",
            "           1       0.99      0.98      0.99       393\n",
            "\n",
            "    accuracy                           1.00     38207\n",
            "   macro avg       0.99      0.99      0.99     38207\n",
            "weighted avg       1.00      1.00      1.00     38207\n",
            "\n",
            "\n",
            "--- Random Forest ---\n",
            "Accuracy: 0.9995\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00     37814\n",
            "           1       1.00      0.96      0.98       393\n",
            "\n",
            "    accuracy                           1.00     38207\n",
            "   macro avg       1.00      0.98      0.99     38207\n",
            "weighted avg       1.00      1.00      1.00     38207\n",
            "\n",
            "\n",
            "--- Logistic Regression ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9941\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00     37814\n",
            "           1       0.77      0.61      0.68       393\n",
            "\n",
            "    accuracy                           0.99     38207\n",
            "   macro avg       0.88      0.80      0.84     38207\n",
            "weighted avg       0.99      0.99      0.99     38207\n",
            "\n",
            "\n",
            "--- Support Vector Machine (SVM) ---\n",
            "Accuracy: 0.9952\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00     37814\n",
            "           1       0.89      0.60      0.72       393\n",
            "\n",
            "    accuracy                           1.00     38207\n",
            "   macro avg       0.95      0.80      0.86     38207\n",
            "weighted avg       0.99      1.00      0.99     38207\n",
            "\n",
            "\n",
            "--- Gaussian Naive Bayes ---\n",
            "Accuracy: 0.7973\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.80      0.89     37814\n",
            "           1       0.05      1.00      0.09       393\n",
            "\n",
            "    accuracy                           0.80     38207\n",
            "   macro avg       0.52      0.90      0.49     38207\n",
            "weighted avg       0.99      0.80      0.88     38207\n",
            "\n",
            "\n",
            "--- Convolutional Neural Network (CNN) ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ conv1d (\u001b[38;5;33mConv1D\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m76\u001b[0m, \u001b[38;5;34m64\u001b[0m)              │             \u001b[38;5;34m256\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ max_pooling1d (\u001b[38;5;33mMaxPooling1D\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m38\u001b[0m, \u001b[38;5;34m64\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ conv1d_1 (\u001b[38;5;33mConv1D\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m36\u001b[0m, \u001b[38;5;34m128\u001b[0m)             │          \u001b[38;5;34m24,704\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ max_pooling1d_1 (\u001b[38;5;33mMaxPooling1D\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m18\u001b[0m, \u001b[38;5;34m128\u001b[0m)             │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2304\u001b[0m)                │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │         \u001b[38;5;34m295,040\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)                   │             \u001b[38;5;34m258\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ conv1d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">76</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)              │             <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ max_pooling1d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">38</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ conv1d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">36</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)             │          <span style=\"color: #00af00; text-decoration-color: #00af00\">24,704</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ max_pooling1d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">18</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)             │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2304</span>)                │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │         <span style=\"color: #00af00; text-decoration-color: #00af00\">295,040</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)                   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">258</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m320,258\u001b[0m (1.22 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">320,258</span> (1.22 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m320,258\u001b[0m (1.22 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">320,258</span> (1.22 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CNN Accuracy: 0.9962\n",
            "\n",
            "Multiclassification using various techniques complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score, classification_report, make_scorer, f1_score\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Reshape\n",
        "\n",
        "# --- **IMPORTANT:** Replace with the actual path to your CSV file ---\n",
        "file_path = '/content/drive/My Drive/CICIDS 2017/Friday-WorkingHours-Morning.pcap_ISCX.csv'\n",
        "# Example: file_path = '/content/drive/My Drive/CICIDS 2017/Monday-workingHours.pcap_ISCX.csv'\n",
        "\n",
        "# --- **SOLUTION:** Correct the target variable column name ---\n",
        "target_column = ' Label'  # **REPLACE 'Label' WITH THE ACTUAL TARGET COLUMN NAME (including space)**\n",
        "\n",
        "try:\n",
        "    data = pd.read_csv(file_path)\n",
        "    print(\"Dataset loaded successfully.\")\n",
        "    print(\"\\nFirst 5 rows of the dataset:\")\n",
        "    print(data.head())\n",
        "\n",
        "    # Separate features (X) and target (y)\n",
        "    if target_column not in data.columns:\n",
        "        print(f\"\\nError: Target variable column '{target_column}' not found in the dataset columns:\")\n",
        "        print(data.columns.tolist())\n",
        "        print(\"Please update the 'target_column' variable with the correct name (including any leading/trailing spaces).\")\n",
        "        exit()\n",
        "    X = data.drop(target_column, axis=1)\n",
        "    y = data[target_column]\n",
        "\n",
        "    # Identify numerical and categorical features\n",
        "    numerical_features = X.select_dtypes(include=np.number).columns\n",
        "    categorical_features = X.select_dtypes(include=['object', 'category']).columns\n",
        "\n",
        "    # --- Preprocessing ---\n",
        "    # Handle missing values (impute numerical with mean, categorical with mode)\n",
        "    for col in numerical_features:\n",
        "        X[col].fillna(X[col].mean(), inplace=True)\n",
        "    for col in categorical_features:\n",
        "        if not X[col].empty:\n",
        "            X[col].fillna(X[col].mode()[0], inplace=True)\n",
        "        else:\n",
        "            print(f\"Warning: Categorical column '{col}' is empty and cannot be imputed with mode.\")\n",
        "\n",
        "    # Encode categorical features\n",
        "    X = pd.get_dummies(X, columns=categorical_features, drop_first=True)\n",
        "\n",
        "    # Split data into training and testing sets with stratification\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "    # Encode the target variable if it's not already numerical\n",
        "    label_encoder = LabelEncoder()\n",
        "    y_train_encoded = label_encoder.fit_transform(y_train)\n",
        "    y_test_encoded = label_encoder.transform(y_test)\n",
        "    num_classes = len(np.unique(y_train_encoded))\n",
        "    print(f\"\\nNumber of classes: {num_classes}\")\n",
        "\n",
        "    # --- Handle potential infinite or very large values before scaling ---\n",
        "    print(\"\\nChecking for infinite or very large values in numerical features...\")\n",
        "    numerical_cols_x_train = X_train.select_dtypes(include=np.number).columns\n",
        "    numerical_cols_x_test = X_test.select_dtypes(include=np.number).columns\n",
        "\n",
        "    for col in numerical_cols_x_train:\n",
        "        X_train[col] = np.where(np.isinf(X_train[col]), np.nan, X_train[col])\n",
        "        X_train[col] = np.where(np.abs(X_train[col]) > 1e10, np.nan, X_train[col])  # Adjust threshold as needed\n",
        "    for col in numerical_cols_x_test:\n",
        "        X_test[col] = np.where(np.isinf(X_test[col]), np.nan, X_test[col])\n",
        "        X_test[col] = np.where(np.abs(X_test[col]) > 1e10, np.nan, X_test[col])  # Adjust threshold as needed\n",
        "\n",
        "    # Impute any remaining NaNs after handling inf/large values\n",
        "    for col in numerical_cols_x_train:\n",
        "        X_train[col].fillna(X_train[col].mean(), inplace=True)\n",
        "    for col in numerical_cols_x_test:\n",
        "        X_test[col].fillna(X_test[col].mean(), inplace=True)\n",
        "\n",
        "    # Scale numerical features\n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X_train)\n",
        "    X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "    # Convert scaled data back to DataFrame for easier handling (optional)\n",
        "    X_train_scaled_df = pd.DataFrame(X_train_scaled, columns=X_train.columns)\n",
        "    X_test_scaled_df = pd.DataFrame(X_test_scaled, columns=X_test.columns)\n",
        "\n",
        "    # --- Multiclassification Models ---\n",
        "\n",
        "    print(\"\\n--- Decision Tree ---\")\n",
        "    dt_classifier = DecisionTreeClassifier(random_state=42)\n",
        "    dt_classifier.fit(X_train_scaled_df, y_train_encoded)\n",
        "    dt_y_pred = dt_classifier.predict(X_test_scaled_df)\n",
        "    dt_accuracy = accuracy_score(y_test_encoded, dt_y_pred)\n",
        "    print(f\"Accuracy: {dt_accuracy:.4f}\")\n",
        "    print(\"Classification Report:\\n\", classification_report(y_test_encoded, dt_y_pred))\n",
        "\n",
        "    print(\"\\n--- Random Forest ---\")\n",
        "    rf_classifier = RandomForestClassifier(random_state=42)\n",
        "    rf_classifier.fit(X_train_scaled_df, y_train_encoded)\n",
        "    rf_y_pred = rf_classifier.predict(X_test_scaled_df)\n",
        "    rf_accuracy = accuracy_score(y_test_encoded, rf_y_pred)\n",
        "    print(f\"Accuracy: {rf_accuracy:.4f}\")\n",
        "    print(\"Classification Report:\\n\", classification_report(y_test_encoded, rf_y_pred))\n",
        "\n",
        "    print(\"\\n--- Logistic Regression ---\")\n",
        "    lr_classifier = LogisticRegression(random_state=42, solver='liblinear', multi_class='ovr', max_iter=1000)\n",
        "    if num_classes >= 2:\n",
        "        lr_classifier.fit(X_train_scaled_df, y_train_encoded)\n",
        "        lr_y_pred = lr_classifier.predict(X_test_scaled_df)\n",
        "        lr_accuracy = accuracy_score(y_test_encoded, lr_y_pred)\n",
        "        print(f\"Accuracy: {lr_accuracy:.4f}\")\n",
        "        print(\"Classification Report:\\n\", classification_report(y_test_encoded, lr_y_pred))\n",
        "    else:\n",
        "        print(\"Warning: Logistic Regression requires at least two classes in the training data. Skipping.\")\n",
        "\n",
        "    print(\"\\n--- Support Vector Machine (SVM) ---\")\n",
        "    svm_classifier = SVC(random_state=42, kernel='rbf', C=1.0)\n",
        "    if num_classes >= 2:\n",
        "        svm_classifier.fit(X_train_scaled_df, y_train_encoded)\n",
        "        svm_y_pred = svm_classifier.predict(X_test_scaled_df)\n",
        "        svm_accuracy = accuracy_score(y_test_encoded, svm_y_pred)\n",
        "        print(f\"Accuracy: {svm_accuracy:.4f}\")\n",
        "        print(\"Classification Report:\\n\", classification_report(y_test_encoded, svm_y_pred))\n",
        "    else:\n",
        "        print(\"Warning: SVM requires at least two classes in the training data. Skipping.\")\n",
        "\n",
        "    print(\"\\n--- Gaussian Naive Bayes ---\")\n",
        "    gnb_classifier = GaussianNB()\n",
        "    gnb_classifier.fit(X_train_scaled_df, y_train_encoded)\n",
        "    gnb_y_pred = gnb_classifier.predict(X_test_scaled_df)\n",
        "    gnb_accuracy = accuracy_score(y_test_encoded, gnb_y_pred)\n",
        "    print(f\"Accuracy: {gnb_accuracy:.4f}\")\n",
        "    print(\"Classification Report:\\n\", classification_report(y_test_encoded, gnb_y_pred))\n",
        "\n",
        "    print(\"\\n--- Convolutional Neural Network (CNN) ---\")\n",
        "    # Reshape input data for CNN (assuming 1 time step)\n",
        "    X_train_cnn = X_train_scaled.reshape(X_train_scaled.shape[0], X_train_scaled.shape[1], 1)\n",
        "    X_test_cnn = X_test_scaled.reshape(X_test_scaled.shape[0], X_test_scaled.shape[1], 1)\n",
        "\n",
        "    cnn_model = keras.Sequential([\n",
        "        Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(X_train_cnn.shape[1], 1)),\n",
        "        MaxPooling1D(pool_size=2),\n",
        "        Conv1D(filters=128, kernel_size=3, activation='relu'),\n",
        "        MaxPooling1D(pool_size=2),\n",
        "        Flatten(),\n",
        "        Dense(128, activation='relu'),\n",
        "        Dense(num_classes, activation='softmax')\n",
        "    ])\n",
        "\n",
        "    cnn_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "    cnn_model.summary()\n",
        "\n",
        "    # Train the CNN\n",
        "    epochs = 10\n",
        "    batch_size = 32\n",
        "    if num_classes >= 2:\n",
        "        cnn_model.fit(X_train_cnn, y_train_encoded, epochs=epochs, batch_size=batch_size, verbose=0)\n",
        "\n",
        "        # Evaluate the CNN\n",
        "        cnn_loss, cnn_accuracy = cnn_model.evaluate(X_test_cnn, y_test_encoded, verbose=0)\n",
        "        print(f\"CNN Accuracy: {cnn_accuracy:.4f}\")\n",
        "    else:\n",
        "        print(\"Warning: CNN requires at least two classes in the training data. Skipping.\")\n",
        "\n",
        "    # --- Performance Estimation using Cross-Validation ---\n",
        "    print(\"\\n--- Performance Estimation using Cross-Validation ---\")\n",
        "\n",
        "    # Example: Evaluate Decision Tree using 5-fold cross-validation and F1 score\n",
        "    scoring = make_scorer(f1_score, average='weighted') # Use weighted average for multiclass\n",
        "    cv_scores_dt = cross_val_score(dt_classifier, X_train_scaled_df, y_train_encoded, cv=5, scoring=scoring)\n",
        "\n",
        "    print(\"\\nDecision Tree Cross-Validation Scores (F1 Weighted):\")\n",
        "    print(cv_scores_dt)\n",
        "    print(f\"Mean F1 Score: {np.mean(cv_scores_dt):.4f}\")\n",
        "    print(f\"Standard Deviation: {np.std(cv_scores_dt):.4f}\")\n",
        "\n",
        "    # Evaluate other models similarly (RandomForest, LogisticRegression, SVM, etc.)\n",
        "\n",
        "    # Example: RandomForest\n",
        "    cv_scores_rf = cross_val_score(rf_classifier, X_train_scaled_df, y_train_encoded, cv=5, scoring=scoring)\n",
        "\n",
        "    print(\"\\nRandom Forest Cross-Validation Scores (F1 Weighted):\")\n",
        "    print(cv_scores_rf)\n",
        "    print(f\"Mean F1 Score: {np.mean(cv_scores_rf):.4f}\")\n",
        "    print(f\"Standard Deviation: {np.std(cv_scores_rf):.4f}\")\n",
        "\n",
        "    # Example: Logistic Regression\n",
        "    if num_classes >= 2:\n",
        "        cv_scores_lr = cross_val_score(lr_classifier, X_train_scaled_df, y_train_encoded, cv=5, scoring=scoring)\n",
        "        print(\"\\nLogistic Regression Cross-Validation Scores (F1 Weighted):\")\n",
        "        print(cv_scores_lr)\n",
        "        print(f\"Mean F1 Score: {np.mean(cv_scores_lr):.4f}\")\n",
        "        print(f\"Standard Deviation: {np.std(cv_scores_lr):.4f}\")\n",
        "    else:\n",
        "        print(\"\\nLogistic Regression Cross-Validation Skipped (less than 2 classes).\")\n",
        "\n",
        "    # Example: Support Vector Machine (SVM)\n",
        "    if num_classes >= 2:\n",
        "        cv_scores_svm = cross_val_score(svm_classifier, X_train_scaled_df, y_train_encoded, cv=5, scoring=scoring)\n",
        "        print(\"\\nSupport Vector Machine Cross-Validation Scores (F1 Weighted):\")\n",
        "        print(cv_scores_svm)\n",
        "        print(f\"Mean F1 Score: {np.mean(cv_scores_svm):.4f}\")\n",
        "        print(f\"Standard Deviation: {np.std(cv_scores_svm):.4f}\")\n",
        "    else:\n",
        "        print(\"\\nSupport Vector Machine Cross-Validation Skipped (less than 2 classes).\")\n",
        "\n",
        "    # Example: Gaussian Naive Bayes\n",
        "    cv_scores_gnb = cross_val_score(gnb_classifier, X_train_scaled_df, y_train_encoded, cv=5, scoring=scoring)\n",
        "    print(\"\\nGaussian Naive Bayes Cross-Validation Scores (F1 Weighted):\")\n",
        "    print(cv_scores_gnb)\n",
        "    print(f\"Mean F1 Score: {np.mean(cv_scores_gnb):.4f}\")\n",
        "    print(f\"Standard Deviation: {np.std(cv_scores_gnb):.4f}\")\n",
        "\n",
        "    print(\"\\nPerformance estimation using cross-validation complete.\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"\\nError: The file '{file_path}' was not found. Please check the file path.\")\n",
        "    exit()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "8dPOiac3m_bJ",
        "outputId": "3f04b0ea-b920-40c7-e31f-072156c15776"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset loaded successfully.\n",
            "\n",
            "First 5 rows of the dataset:\n",
            "    Destination Port   Flow Duration   Total Fwd Packets  \\\n",
            "0               3268       112740690                  32   \n",
            "1                389       112740560                  32   \n",
            "2                  0       113757377                 545   \n",
            "3               5355          100126                  22   \n",
            "4                  0           54760                   4   \n",
            "\n",
            "    Total Backward Packets  Total Length of Fwd Packets  \\\n",
            "0                       16                         6448   \n",
            "1                       16                         6448   \n",
            "2                        0                            0   \n",
            "3                        0                          616   \n",
            "4                        0                            0   \n",
            "\n",
            "    Total Length of Bwd Packets   Fwd Packet Length Max  \\\n",
            "0                          1152                     403   \n",
            "1                          5056                     403   \n",
            "2                             0                       0   \n",
            "3                             0                      28   \n",
            "4                             0                       0   \n",
            "\n",
            "    Fwd Packet Length Min   Fwd Packet Length Mean   Fwd Packet Length Std  \\\n",
            "0                       0                    201.5              204.724205   \n",
            "1                       0                    201.5              204.724205   \n",
            "2                       0                      0.0                0.000000   \n",
            "3                      28                     28.0                0.000000   \n",
            "4                       0                      0.0                0.000000   \n",
            "\n",
            "   ...   min_seg_size_forward   Active Mean    Active Std   Active Max  \\\n",
            "0  ...                     32  3.594286e+02  1.199802e+01          380   \n",
            "1  ...                     32  3.202857e+02  1.574499e+01          330   \n",
            "2  ...                      0  9.361829e+06  7.324646e+06     18900000   \n",
            "3  ...                     32  0.000000e+00  0.000000e+00            0   \n",
            "4  ...                      0  0.000000e+00  0.000000e+00            0   \n",
            "\n",
            "    Active Min   Idle Mean      Idle Std   Idle Max   Idle Min   Label  \n",
            "0          343  16100000.0  4.988048e+05   16400000   15400000  BENIGN  \n",
            "1          285  16100000.0  4.987937e+05   16400000   15400000  BENIGN  \n",
            "2           19  12200000.0  6.935824e+06   20800000    5504997  BENIGN  \n",
            "3            0         0.0  0.000000e+00          0          0  BENIGN  \n",
            "4            0         0.0  0.000000e+00          0          0  BENIGN  \n",
            "\n",
            "[5 rows x 79 columns]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-11-c02f2455fd52>:44: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  X[col].fillna(X[col].mean(), inplace=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Number of classes: 2\n",
            "\n",
            "Checking for infinite or very large values in numerical features...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-11-c02f2455fd52>:78: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  X_train[col].fillna(X_train[col].mean(), inplace=True)\n",
            "<ipython-input-11-c02f2455fd52>:80: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  X_test[col].fillna(X_test[col].mean(), inplace=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Decision Tree ---\n",
            "Accuracy: 0.9997\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00     37814\n",
            "           1       0.99      0.98      0.99       393\n",
            "\n",
            "    accuracy                           1.00     38207\n",
            "   macro avg       0.99      0.99      0.99     38207\n",
            "weighted avg       1.00      1.00      1.00     38207\n",
            "\n",
            "\n",
            "--- Random Forest ---\n",
            "Accuracy: 0.9995\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00     37814\n",
            "           1       1.00      0.96      0.98       393\n",
            "\n",
            "    accuracy                           1.00     38207\n",
            "   macro avg       1.00      0.98      0.99     38207\n",
            "weighted avg       1.00      1.00      1.00     38207\n",
            "\n",
            "\n",
            "--- Logistic Regression ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9941\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00     37814\n",
            "           1       0.77      0.61      0.68       393\n",
            "\n",
            "    accuracy                           0.99     38207\n",
            "   macro avg       0.88      0.80      0.84     38207\n",
            "weighted avg       0.99      0.99      0.99     38207\n",
            "\n",
            "\n",
            "--- Support Vector Machine (SVM) ---\n",
            "Accuracy: 0.9952\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00     37814\n",
            "           1       0.89      0.60      0.72       393\n",
            "\n",
            "    accuracy                           1.00     38207\n",
            "   macro avg       0.95      0.80      0.86     38207\n",
            "weighted avg       0.99      1.00      0.99     38207\n",
            "\n",
            "\n",
            "--- Gaussian Naive Bayes ---\n",
            "Accuracy: 0.7973\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.80      0.89     37814\n",
            "           1       0.05      1.00      0.09       393\n",
            "\n",
            "    accuracy                           0.80     38207\n",
            "   macro avg       0.52      0.90      0.49     38207\n",
            "weighted avg       0.99      0.80      0.88     38207\n",
            "\n",
            "\n",
            "--- Convolutional Neural Network (CNN) ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ conv1d_2 (\u001b[38;5;33mConv1D\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m76\u001b[0m, \u001b[38;5;34m64\u001b[0m)              │             \u001b[38;5;34m256\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ max_pooling1d_2 (\u001b[38;5;33mMaxPooling1D\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m38\u001b[0m, \u001b[38;5;34m64\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ conv1d_3 (\u001b[38;5;33mConv1D\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m36\u001b[0m, \u001b[38;5;34m128\u001b[0m)             │          \u001b[38;5;34m24,704\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ max_pooling1d_3 (\u001b[38;5;33mMaxPooling1D\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m18\u001b[0m, \u001b[38;5;34m128\u001b[0m)             │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ flatten_1 (\u001b[38;5;33mFlatten\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2304\u001b[0m)                │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │         \u001b[38;5;34m295,040\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)                   │             \u001b[38;5;34m258\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ conv1d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">76</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)              │             <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ max_pooling1d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">38</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ conv1d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">36</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)             │          <span style=\"color: #00af00; text-decoration-color: #00af00\">24,704</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ max_pooling1d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">18</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)             │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ flatten_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2304</span>)                │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │         <span style=\"color: #00af00; text-decoration-color: #00af00\">295,040</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)                   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">258</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m320,258\u001b[0m (1.22 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">320,258</span> (1.22 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m320,258\u001b[0m (1.22 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">320,258</span> (1.22 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CNN Accuracy: 0.9962\n",
            "\n",
            "--- Performance Estimation using Cross-Validation ---\n",
            "\n",
            "Decision Tree Cross-Validation Scores (F1 Weighted):\n",
            "[0.99957635 0.99960862 0.99960801 0.99947897 0.99957434]\n",
            "Mean F1 Score: 0.9996\n",
            "Standard Deviation: 0.0000\n",
            "\n",
            "Random Forest Cross-Validation Scores (F1 Weighted):\n",
            "[0.99926857 0.99927446 0.999202   0.99936881 0.99943617]\n",
            "Mean F1 Score: 0.9993\n",
            "Standard Deviation: 0.0001\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Logistic Regression Cross-Validation Scores (F1 Weighted):\n",
            "[0.99395563 0.9943786  0.99420558 0.99373241 0.9944497 ]\n",
            "Mean F1 Score: 0.9941\n",
            "Standard Deviation: 0.0003\n",
            "\n",
            "Support Vector Machine Cross-Validation Scores (F1 Weighted):\n",
            "[0.99471397 0.99504058 0.99482524 0.99444483 0.99513437]\n",
            "Mean F1 Score: 0.9948\n",
            "Standard Deviation: 0.0002\n",
            "\n",
            "Gaussian Naive Bayes Cross-Validation Scores (F1 Weighted):\n",
            "[0.88536625 0.875006   0.87578589 0.87747737 0.87876447]\n",
            "Mean F1 Score: 0.8785\n",
            "Standard Deviation: 0.0037\n",
            "\n",
            "Performance estimation using cross-validation complete.\n"
          ]
        }
      ]
    }
  ]
}